{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm as SN\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../DATA/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"Time\", \"Class\"],axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "num_scaler = mms(feature_range=(-1,1))\n",
    "columns = data.columns.tolist()\n",
    "data[columns] = num_scaler.fit_transform(data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.870385</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.762730</td>\n",
       "      <td>-0.373955</td>\n",
       "      <td>0.526877</td>\n",
       "      <td>-0.464663</td>\n",
       "      <td>-0.466370</td>\n",
       "      <td>0.572888</td>\n",
       "      <td>-0.049377</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165884</td>\n",
       "      <td>0.122369</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>-0.217495</td>\n",
       "      <td>0.170244</td>\n",
       "      <td>-0.210886</td>\n",
       "      <td>-0.162048</td>\n",
       "      <td>-0.374607</td>\n",
       "      <td>-0.988352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957084</td>\n",
       "      <td>0.540133</td>\n",
       "      <td>0.680597</td>\n",
       "      <td>-0.456407</td>\n",
       "      <td>0.532241</td>\n",
       "      <td>-0.475617</td>\n",
       "      <td>-0.470249</td>\n",
       "      <td>0.572597</td>\n",
       "      <td>-0.092038</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159060</td>\n",
       "      <td>0.115680</td>\n",
       "      <td>-0.039526</td>\n",
       "      <td>0.333876</td>\n",
       "      <td>-0.327120</td>\n",
       "      <td>0.174581</td>\n",
       "      <td>-0.107974</td>\n",
       "      <td>-0.167310</td>\n",
       "      <td>-0.373155</td>\n",
       "      <td>-0.999791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.870434</td>\n",
       "      <td>0.506235</td>\n",
       "      <td>0.736282</td>\n",
       "      <td>-0.462469</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>-0.437756</td>\n",
       "      <td>-0.459646</td>\n",
       "      <td>0.576085</td>\n",
       "      <td>-0.178795</td>\n",
       "      <td>0.026036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171710</td>\n",
       "      <td>0.130955</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.357878</td>\n",
       "      <td>-0.421292</td>\n",
       "      <td>0.119030</td>\n",
       "      <td>-0.194546</td>\n",
       "      <td>-0.169021</td>\n",
       "      <td>-0.376177</td>\n",
       "      <td>-0.970522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.883756</td>\n",
       "      <td>0.530608</td>\n",
       "      <td>0.736967</td>\n",
       "      <td>-0.572678</td>\n",
       "      <td>0.531294</td>\n",
       "      <td>-0.448882</td>\n",
       "      <td>-0.466394</td>\n",
       "      <td>0.578868</td>\n",
       "      <td>-0.170001</td>\n",
       "      <td>0.015170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.119467</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.325214</td>\n",
       "      <td>-0.552348</td>\n",
       "      <td>0.228491</td>\n",
       "      <td>-0.221607</td>\n",
       "      <td>-0.164663</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.990386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877234</td>\n",
       "      <td>0.553040</td>\n",
       "      <td>0.728501</td>\n",
       "      <td>-0.460407</td>\n",
       "      <td>0.525950</td>\n",
       "      <td>-0.472032</td>\n",
       "      <td>-0.462064</td>\n",
       "      <td>0.564967</td>\n",
       "      <td>-0.018101</td>\n",
       "      <td>0.048606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.122655</td>\n",
       "      <td>0.094541</td>\n",
       "      <td>0.326784</td>\n",
       "      <td>-0.197460</td>\n",
       "      <td>0.132685</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>-0.158878</td>\n",
       "      <td>-0.365020</td>\n",
       "      <td>-0.994551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.870385  0.532981  0.762730 -0.373955  0.526877 -0.464663 -0.466370   \n",
       "1  0.957084  0.540133  0.680597 -0.456407  0.532241 -0.475617 -0.470249   \n",
       "2  0.870434  0.506235  0.736282 -0.462469  0.524658 -0.437756 -0.459646   \n",
       "3  0.883756  0.530608  0.736967 -0.572678  0.531294 -0.448882 -0.466394   \n",
       "4  0.877234  0.553040  0.728501 -0.460407  0.525950 -0.472032 -0.462064   \n",
       "\n",
       "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0  0.572888 -0.049377  0.021201  ...  0.165884  0.122369  0.045984  0.327586   \n",
       "1  0.572597 -0.092038  0.010535  ...  0.159060  0.115680 -0.039526  0.333876   \n",
       "2  0.576085 -0.178795  0.026036  ...  0.171710  0.130955  0.092060  0.357878   \n",
       "3  0.578868 -0.170001  0.015170  ...  0.156100  0.119467  0.020554  0.325214   \n",
       "4  0.564967 -0.018101  0.048606  ...  0.169231  0.122655  0.094541  0.326784   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  \n",
       "0 -0.217495  0.170244 -0.210886 -0.162048 -0.374607 -0.988352  \n",
       "1 -0.327120  0.174581 -0.107974 -0.167310 -0.373155 -0.999791  \n",
       "2 -0.421292  0.119030 -0.194546 -0.169021 -0.376177 -0.970522  \n",
       "3 -0.552348  0.228491 -0.221607 -0.164663 -0.371258 -0.990386  \n",
       "4 -0.197460  0.132685  0.014994 -0.158878 -0.365020 -0.994551  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module) :\n",
    "    def __init__(self,layers) :\n",
    "        super().__init__()\n",
    "        self.fc = self.make_model(layers,sn=True,bn=True,dropout=False)\n",
    "        self.fc.apply(self.init_weights)\n",
    "        self.num_scaler = nn.Tanh()\n",
    "    \n",
    "    def forward(self ,x ):\n",
    "        x = self.fc(x)\n",
    "        x = self.num_scaler(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.0)\n",
    "            \n",
    "    def make_model(self, layers , sn=True, bn =False,dropout=True) :\n",
    "        model = []\n",
    "        for idx , layer in enumerate(layers[1:]) :\n",
    "            mod = nn.Linear(layers[idx], layer)\n",
    "            if sn == True :\n",
    "                mod = SN(mod)\n",
    "            model.append(mod)\n",
    "            if (idx+1) == len(layers[1:]) :\n",
    "                pass\n",
    "            else :\n",
    "                if dropout == True :\n",
    "                    model.append(nn.AlphaDropout(0.8))\n",
    "                if bn == True :\n",
    "                    model.append(nn.BatchNorm1d(layer))\n",
    "                model.append(nn.SELU())\n",
    "        return nn.Sequential(*model)\n",
    "    \n",
    "class Discriminator(nn.Module) :\n",
    "    def __init__(self,layers) :\n",
    "        super().__init__()\n",
    "        self.fc = self.make_model(layers,sn=True,bn=True,dropout=False)\n",
    "        self.fc.apply(self.init_weights)\n",
    "        self.num_scaler = nn.Tanh()\n",
    "    \n",
    "    def forward(self ,x ):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self,m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.0)\n",
    "            \n",
    "    def make_model(self, layers , sn=True, bn =False,dropout=True) :\n",
    "        model = []\n",
    "        for idx , layer in enumerate(layers[1:]) :\n",
    "            mod = nn.Linear(layers[idx], layer)\n",
    "            if sn == True :\n",
    "                mod = SN(mod)\n",
    "            model.append(mod)\n",
    "            if (idx+1) == len(layers[1:]) :\n",
    "                pass\n",
    "            else :\n",
    "                if dropout == True :\n",
    "                    model.append(nn.AlphaDropout(0.8))\n",
    "                if bn == True :\n",
    "                    model.append(nn.BatchNorm1d(layer))\n",
    "                model.append(nn.SELU())\n",
    "        return nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "class TabularDataset(Dataset) :\n",
    "    def __init__(self, X) :\n",
    "        self.X = X\n",
    "    def __len__(self) :\n",
    "        return len(self.X)\n",
    "    def __getitem__(self,idx) :\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "        return self.X[idx,:].astype(np.float32)\n",
    "    \n",
    "class origian_gan(object)  :\n",
    "    def __init__(self,) :\n",
    "        self.adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "        self.store = {\"epoch\" : [], \"dloss\" : [], \"gloss\" : []}\n",
    "        self.load_n = 0\n",
    "        \n",
    "    def build_graph(self, g_args , num_scaler) :\n",
    "        self.latent_dim , g_layers , d_layers , g_lr, d_lr, b1, b2, cuda = g_args\n",
    "        self.g_args = [self.latent_dim , g_layers , d_layers]\n",
    "        self.num_scaler = num_scaler\n",
    "        self.generator= Generator(layers=g_layers)\n",
    "        self.discriminator= Discriminator(layers=d_layers)\n",
    "        if cuda:\n",
    "            self.generator.cuda()\n",
    "            self.discriminator.cuda()\n",
    "            self.adversarial_loss.cuda()\n",
    "        self.optimG = torch.optim.Adam(self.generator.parameters(), \n",
    "                                       lr=g_lr, betas=(b1, b2))\n",
    "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), \n",
    "                                       lr=d_lr, betas=(b1, b2))\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    \n",
    "    def sample_z(self,row) :\n",
    "        z = Variable(self.Tensor(np.random.normal(0, 1, (row, self.latent_dim))))\n",
    "        return z\n",
    "    \n",
    "    def inference(self,row , cuda = False , load_path = None) :\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        if load_path is not None :\n",
    "            if self.load_n == 0 :\n",
    "                self.load_model(load_path)\n",
    "                self.load_n = 1\n",
    "        z = self.sample_z(row)\n",
    "        result = self.generator(z).detach().numpy()\n",
    "        return result\n",
    "        \n",
    "    def generate_label_z(self, row ) :\n",
    "        valid = Variable(self.Tensor(row, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(self.Tensor(row, 1).fill_(0.0), requires_grad=False)\n",
    "        z = self.sample_z(row)\n",
    "        return valid , fake , z\n",
    "        \n",
    "    def train(self, data , g_args , num_scaler , n_epochs = 100, batch_size = 500) :\n",
    "        self.build_graph(g_args , num_scaler)\n",
    "        trainset = TabularDataset(data.values)\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, \n",
    "                                 shuffle=True,num_workers=20)\n",
    "        for epoch in range(n_epochs):\n",
    "            for i, batch_data in enumerate(trainloader):\n",
    "                real_data = Variable(batch_data.type(self.Tensor))\n",
    "                valid , fake , z = self.generate_label_z(batch_data.size()[0])\n",
    "                self.optimG.zero_grad()\n",
    "                gen_data = self.generator(z)\n",
    "                g_loss = self.adversarial_loss(self.discriminator(gen_data), valid)\n",
    "                g_loss.backward()\n",
    "                self.optimG.step()\n",
    "                self.optimD.zero_grad()\n",
    "                real_loss = self.adversarial_loss(self.discriminator(real_data), valid)\n",
    "                fake_loss = self.adversarial_loss(self.discriminator(gen_data.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                self.optimD.step()\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D loss: %.4f] [G loss: %.4f]\"\n",
    "                    % (epoch, n_epochs, i, len(trainloader), d_loss.item(), g_loss.item())\n",
    "                , end=\"\\r\")\n",
    "                \n",
    "            eval_data = self.Tensor(data.values)\n",
    "            valid , fake , z = self.generate_label_z(eval_data.size()[0])\n",
    "            gen_data = self.generator(z)\n",
    "            g_loss = self.adversarial_loss(self.discriminator(gen_data), valid)\n",
    "            real_loss = self.adversarial_loss(self.discriminator(eval_data), valid)\n",
    "            fake_loss = self.adversarial_loss(self.discriminator(gen_data),fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            self.store[\"epoch\"].append(epoch)\n",
    "            self.store[\"dloss\"].append(d_loss.item())\n",
    "            self.store[\"gloss\"].append(g_loss.item())\n",
    "            self.vis()\n",
    "            self.save_model(save_path=\"./gan_model.pth\")\n",
    "    \n",
    "    def vis(self,) :\n",
    "        clear_output()\n",
    "        plt.plot(self.store[\"epoch\"], self.store[\"dloss\"], label =\"dloss\")\n",
    "        plt.plot(self.store[\"epoch\"], self.store[\"gloss\"], label =\"gloss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self,save_path) :\n",
    "        save_path = \"./gan_model.pth\"\n",
    "        info = {\n",
    "            \"g_args\" : self.g_args , \n",
    "            \"num_scaler\" : self.num_scaler,\n",
    "            \"generator_state_dict\" : self.generator.state_dict(),\n",
    "            \"discriminator_state_dict\" : self.discriminator.state_dict(),\n",
    "            \"generator_optim_state_dict\" : self.optimG.state_dict(),\n",
    "            \"discriminator_optim_state_dict\" : self.optimD.state_dict()}\n",
    "        torch.save(info, save_path)\n",
    "    \n",
    "    def load_model(self,load_path = \"./gan_model.pth\") :\n",
    "        self.checkpoint = torch.load(load_path)\n",
    "        self.latent_dim , g_layers, d_layers = self.checkpoint[\"g_args\"]\n",
    "        self.num_scaler = self.checkpoint[\"num_scaler\"]\n",
    "        self.generator = Generator(layers = g_layers)\n",
    "        self.discriminator = Discriminator(layers = d_layers)\n",
    "        \n",
    "        self.discriminator.load_state_dict(self.checkpoint[\"discriminator_state_dict\"])\n",
    "        self.generator.load_state_dict(self.checkpoint[\"generator_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_gan = origian_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 126/950] [D loss: 0.7430] [G loss: 0.6679]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5f181b3f5593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mg_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_layers\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0md_layers\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mg_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mori_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_args\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e4d225350593>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, g_args, num_scaler, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mgen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_dim = 10\n",
    "total_col_n = len(columns)\n",
    "g_layers = [latent_dim, 20, 30, total_col_n]\n",
    "d_layers = [total_col_n , 20 ,10,1]\n",
    "g_lr = 1e-4\n",
    "d_lr = 1e-5\n",
    "b1 , b2 = 0.5 , 0.999\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "g_args = [latent_dim, g_layers , d_layers , g_lr, d_lr, b1, b2, cuda] \n",
    "scaler = num_scaler\n",
    "ori_gan.train(data, g_args , scaler, n_epochs=200, batch_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ori_gan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-37f4e0c2ad44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mori_gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mori_gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ori_gan' is not defined"
     ]
    }
   ],
   "source": [
    "del ori_gan\n",
    "ori_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_gan = origian_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.971527  , -0.58575076, -0.7673694 , ...,  0.45700347,\n",
       "         0.28068584, -0.5485998 ],\n",
       "       [-0.33350462,  0.07272428,  0.03030981, ...,  0.33195886,\n",
       "         0.41954464,  0.61596596],\n",
       "       [-0.42303717,  0.64501405,  0.43706402, ..., -0.18789387,\n",
       "        -0.21473606,  0.6313253 ],\n",
       "       ...,\n",
       "       [-0.24014457,  0.62147963,  0.4474559 , ...,  0.26185125,\n",
       "        -0.5990192 , -0.03740655],\n",
       "       [-0.64913243, -0.08135279,  0.7817026 , ..., -0.37349516,\n",
       "        -0.3349383 ,  0.2207043 ],\n",
       "       [-0.26652536, -0.18160778, -0.3385083 , ...,  0.41758123,\n",
       "        -0.64169693, -0.78257155]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_gan.inference(row = 100  , cuda = cuda, load_path=\"./gan_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
