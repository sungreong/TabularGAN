{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm as SN\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../DATA/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"Time\", \"Class\"],axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "num_scaler = mms(feature_range=(-1,1))\n",
    "columns = data.columns.tolist()\n",
    "data[columns] = num_scaler.fit_transform(data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.870385</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.762730</td>\n",
       "      <td>-0.373955</td>\n",
       "      <td>0.526877</td>\n",
       "      <td>-0.464663</td>\n",
       "      <td>-0.466370</td>\n",
       "      <td>0.572888</td>\n",
       "      <td>-0.049377</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165884</td>\n",
       "      <td>0.122369</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>-0.217495</td>\n",
       "      <td>0.170244</td>\n",
       "      <td>-0.210886</td>\n",
       "      <td>-0.162048</td>\n",
       "      <td>-0.374607</td>\n",
       "      <td>-0.988352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957084</td>\n",
       "      <td>0.540133</td>\n",
       "      <td>0.680597</td>\n",
       "      <td>-0.456407</td>\n",
       "      <td>0.532241</td>\n",
       "      <td>-0.475617</td>\n",
       "      <td>-0.470249</td>\n",
       "      <td>0.572597</td>\n",
       "      <td>-0.092038</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159060</td>\n",
       "      <td>0.115680</td>\n",
       "      <td>-0.039526</td>\n",
       "      <td>0.333876</td>\n",
       "      <td>-0.327120</td>\n",
       "      <td>0.174581</td>\n",
       "      <td>-0.107974</td>\n",
       "      <td>-0.167310</td>\n",
       "      <td>-0.373155</td>\n",
       "      <td>-0.999791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.870434</td>\n",
       "      <td>0.506235</td>\n",
       "      <td>0.736282</td>\n",
       "      <td>-0.462469</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>-0.437756</td>\n",
       "      <td>-0.459646</td>\n",
       "      <td>0.576085</td>\n",
       "      <td>-0.178795</td>\n",
       "      <td>0.026036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171710</td>\n",
       "      <td>0.130955</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.357878</td>\n",
       "      <td>-0.421292</td>\n",
       "      <td>0.119030</td>\n",
       "      <td>-0.194546</td>\n",
       "      <td>-0.169021</td>\n",
       "      <td>-0.376177</td>\n",
       "      <td>-0.970522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.883756</td>\n",
       "      <td>0.530608</td>\n",
       "      <td>0.736967</td>\n",
       "      <td>-0.572678</td>\n",
       "      <td>0.531294</td>\n",
       "      <td>-0.448882</td>\n",
       "      <td>-0.466394</td>\n",
       "      <td>0.578868</td>\n",
       "      <td>-0.170001</td>\n",
       "      <td>0.015170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.119467</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.325214</td>\n",
       "      <td>-0.552348</td>\n",
       "      <td>0.228491</td>\n",
       "      <td>-0.221607</td>\n",
       "      <td>-0.164663</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.990386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877234</td>\n",
       "      <td>0.553040</td>\n",
       "      <td>0.728501</td>\n",
       "      <td>-0.460407</td>\n",
       "      <td>0.525950</td>\n",
       "      <td>-0.472032</td>\n",
       "      <td>-0.462064</td>\n",
       "      <td>0.564967</td>\n",
       "      <td>-0.018101</td>\n",
       "      <td>0.048606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.122655</td>\n",
       "      <td>0.094541</td>\n",
       "      <td>0.326784</td>\n",
       "      <td>-0.197460</td>\n",
       "      <td>0.132685</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>-0.158878</td>\n",
       "      <td>-0.365020</td>\n",
       "      <td>-0.994551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.870385  0.532981  0.762730 -0.373955  0.526877 -0.464663 -0.466370   \n",
       "1  0.957084  0.540133  0.680597 -0.456407  0.532241 -0.475617 -0.470249   \n",
       "2  0.870434  0.506235  0.736282 -0.462469  0.524658 -0.437756 -0.459646   \n",
       "3  0.883756  0.530608  0.736967 -0.572678  0.531294 -0.448882 -0.466394   \n",
       "4  0.877234  0.553040  0.728501 -0.460407  0.525950 -0.472032 -0.462064   \n",
       "\n",
       "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0  0.572888 -0.049377  0.021201  ...  0.165884  0.122369  0.045984  0.327586   \n",
       "1  0.572597 -0.092038  0.010535  ...  0.159060  0.115680 -0.039526  0.333876   \n",
       "2  0.576085 -0.178795  0.026036  ...  0.171710  0.130955  0.092060  0.357878   \n",
       "3  0.578868 -0.170001  0.015170  ...  0.156100  0.119467  0.020554  0.325214   \n",
       "4  0.564967 -0.018101  0.048606  ...  0.169231  0.122655  0.094541  0.326784   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  \n",
       "0 -0.217495  0.170244 -0.210886 -0.162048 -0.374607 -0.988352  \n",
       "1 -0.327120  0.174581 -0.107974 -0.167310 -0.373155 -0.999791  \n",
       "2 -0.421292  0.119030 -0.194546 -0.169021 -0.376177 -0.970522  \n",
       "3 -0.552348  0.228491 -0.221607 -0.164663 -0.371258 -0.990386  \n",
       "4 -0.197460  0.132685  0.014994 -0.158878 -0.365020 -0.994551  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module) :\n",
    "    def __init__(self,layers) :\n",
    "        super().__init__()\n",
    "        self.fc = self.make_model(layers,sn=True,bn=True,dropout=False)\n",
    "        self.fc.apply(self.init_weights)\n",
    "        self.num_scaler = nn.Tanh()\n",
    "    \n",
    "    def forward(self ,x ):\n",
    "        x = self.fc(x)\n",
    "        x = self.num_scaler(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.0)\n",
    "            \n",
    "    def make_model(self, layers , sn=True, bn =False,dropout=True) :\n",
    "        model = []\n",
    "        for idx , layer in enumerate(layers[1:]) :\n",
    "            mod = nn.Linear(layers[idx], layer)\n",
    "            if sn == True :\n",
    "                mod = SN(mod)\n",
    "            model.append(mod)\n",
    "            if (idx+1) == len(layers[1:]) :\n",
    "                pass\n",
    "            else :\n",
    "                if dropout == True :\n",
    "                    model.append(nn.AlphaDropout(0.8))\n",
    "                if bn == True :\n",
    "                    model.append(nn.BatchNorm1d(layer))\n",
    "                model.append(nn.SELU())\n",
    "        return nn.Sequential(*model)\n",
    "    \n",
    "class Discriminator(nn.Module) :\n",
    "    def __init__(self,layers) :\n",
    "        super().__init__()\n",
    "        self.fc = self.make_model(layers,sn=True,bn=True,dropout=False)\n",
    "        self.fc.apply(self.init_weights)\n",
    "        self.num_scaler = nn.Tanh()\n",
    "    \n",
    "    def forward(self ,x ):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self,m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.0)\n",
    "            \n",
    "    def make_model(self, layers , sn=True, bn =False,dropout=True) :\n",
    "        model = []\n",
    "        for idx , layer in enumerate(layers[1:]) :\n",
    "            mod = nn.Linear(layers[idx], layer)\n",
    "            if sn == True :\n",
    "                mod = SN(mod)\n",
    "            model.append(mod)\n",
    "            if (idx+1) == len(layers[1:]) :\n",
    "                pass\n",
    "            else :\n",
    "                if dropout == True :\n",
    "                    model.append(nn.AlphaDropout(0.8))\n",
    "                if bn == True :\n",
    "                    model.append(nn.BatchNorm1d(layer))\n",
    "                model.append(nn.SELU())\n",
    "        return nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "class TabularDataset(Dataset) :\n",
    "    def __init__(self, X) :\n",
    "        self.X = X\n",
    "    def __len__(self) :\n",
    "        return len(self.X)\n",
    "    def __getitem__(self,idx) :\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "        return self.X[idx,:].astype(np.float32)\n",
    "    \n",
    "class origian_gan(object)  :\n",
    "    def __init__(self,) :\n",
    "        self.adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "        self.store = {\"epoch\" : [], \"dloss\" : [], \"gloss\" : []}\n",
    "        self.load_n = 0\n",
    "        \n",
    "    def build_graph(self, g_args) :\n",
    "        self.latent_dim , g_layers , d_layers , g_lr, d_lr, b1, b2, cuda = g_args\n",
    "        self.g_args = [self.latent_dim , g_layers , d_layers]\n",
    "        self.generator= Generator(layers=g_layers)\n",
    "        self.discriminator= Discriminator(layers=d_layers)\n",
    "        if cuda:\n",
    "            self.generator.cuda()\n",
    "            self.discriminator.cuda()\n",
    "            self.adversarial_loss.cuda()\n",
    "        self.optimG = torch.optim.Adam(self.generator.parameters(), \n",
    "                                       lr=g_lr, betas=(b1, b2))\n",
    "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), \n",
    "                                       lr=d_lr, betas=(b1, b2))\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    \n",
    "    def sample_z(self,row) :\n",
    "        z = Variable(self.Tensor(np.random.normal(0, 1, (row, self.latent_dim))))\n",
    "        return z\n",
    "    \n",
    "    def inference(self,row , cuda = False , load_path = None) :\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        if load_path is not None :\n",
    "            if self.load_n == 0 :\n",
    "                self.load_model(load_path)\n",
    "                self.load_n = 1\n",
    "        z = self.sample_z(row)\n",
    "        result = self.generator(z).detach().numpy()\n",
    "        pd_result = pd.DataFrame(result ,columns = self.columns)\n",
    "        pd_result[self.columns] =\\\n",
    "        self.num_scaler.inverse_transform(pd_result[self.columns])\n",
    "        return pd_result\n",
    "        \n",
    "    def generate_label_z(self, row ) :\n",
    "        valid = Variable(self.Tensor(row, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(self.Tensor(row, 1).fill_(0.0), requires_grad=False)\n",
    "        z = self.sample_z(row)\n",
    "        return valid , fake , z\n",
    "        \n",
    "    def train(self, data , g_args , num_scaler , columns, n_epochs = 100, batch_size = 500) :\n",
    "        self.build_graph(g_args)\n",
    "        self.num_scaler = num_scaler\n",
    "        self.columns = columns\n",
    "        trainset = TabularDataset(data.values)\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, \n",
    "                                 shuffle=True,num_workers=20)\n",
    "        for epoch in range(n_epochs):\n",
    "            for i, batch_data in enumerate(trainloader):\n",
    "                real_data = Variable(batch_data.type(self.Tensor))\n",
    "                valid , fake , z = self.generate_label_z(batch_data.size()[0])\n",
    "                self.optimG.zero_grad()\n",
    "                gen_data = self.generator(z)\n",
    "                g_loss = self.adversarial_loss(self.discriminator(gen_data), valid)\n",
    "                g_loss.backward()\n",
    "                self.optimG.step()\n",
    "                self.optimD.zero_grad()\n",
    "                real_loss = self.adversarial_loss(self.discriminator(real_data), valid)\n",
    "                fake_loss = self.adversarial_loss(self.discriminator(gen_data.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                self.optimD.step()\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D loss: %.4f] [G loss: %.4f]\"\n",
    "                    % (epoch, n_epochs, i, len(trainloader), d_loss.item(), g_loss.item())\n",
    "                , end=\"\\r\")\n",
    "                \n",
    "            eval_data = self.Tensor(data.values)\n",
    "            valid , fake , z = self.generate_label_z(eval_data.size()[0])\n",
    "            gen_data = self.generator(z)\n",
    "            g_loss = self.adversarial_loss(self.discriminator(gen_data), valid)\n",
    "            real_loss = self.adversarial_loss(self.discriminator(eval_data), valid)\n",
    "            fake_loss = self.adversarial_loss(self.discriminator(gen_data),fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            self.store[\"epoch\"].append(epoch)\n",
    "            self.store[\"dloss\"].append(d_loss.item())\n",
    "            self.store[\"gloss\"].append(g_loss.item())\n",
    "            self.vis()\n",
    "            self.save_model(save_path=\"./gan_model.pth\")\n",
    "    \n",
    "    def vis(self,) :\n",
    "        clear_output()\n",
    "        plt.plot(self.store[\"epoch\"], self.store[\"dloss\"], label =\"dloss\")\n",
    "        plt.plot(self.store[\"epoch\"], self.store[\"gloss\"], label =\"gloss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self,save_path) :\n",
    "        save_path = \"./gan_model.pth\"\n",
    "        info = {\n",
    "            \"g_args\" : self.g_args , \n",
    "            \"num_scaler\" : self.num_scaler,\n",
    "            \"columns\" : self.columns,\n",
    "            \"generator_state_dict\" : self.generator.state_dict(),\n",
    "            \"discriminator_state_dict\" : self.discriminator.state_dict(),\n",
    "            \"generator_optim_state_dict\" : self.optimG.state_dict(),\n",
    "            \"discriminator_optim_state_dict\" : self.optimD.state_dict()}\n",
    "        torch.save(info, save_path)\n",
    "    \n",
    "    def load_model(self,load_path = \"./gan_model.pth\") :\n",
    "        self.checkpoint = torch.load(load_path)\n",
    "        self.latent_dim , g_layers, d_layers = self.checkpoint[\"g_args\"]\n",
    "        self.num_scaler = self.checkpoint[\"num_scaler\"]\n",
    "        self.columns = self.checkpoint[\"columns\"]\n",
    "        self.generator = Generator(layers = g_layers)\n",
    "        self.discriminator = Discriminator(layers = d_layers)\n",
    "        self.discriminator.load_state_dict(self.checkpoint[\"discriminator_state_dict\"])\n",
    "        self.generator.load_state_dict(self.checkpoint[\"generator_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_gan = origian_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 113/200] [Batch 321/475] [D loss: 0.7003] [G loss: 0.6903]\r"
     ]
    }
   ],
   "source": [
    "latent_dim = 10\n",
    "total_col_n = len(columns)\n",
    "g_layers = [latent_dim, 20, 30, total_col_n]\n",
    "d_layers = [total_col_n , 20 ,10,1]\n",
    "g_lr = 1e-4\n",
    "d_lr = 1e-5\n",
    "b1 , b2 = 0.5 , 0.999\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "g_args = [latent_dim, g_layers , d_layers , g_lr, d_lr, b1, b2, cuda] \n",
    "scaler = num_scaler\n",
    "ori_gan.train(data, g_args , scaler, columns , n_epochs=200, batch_size = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ori_gan\n",
    "ori_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_gan = origian_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-32.892689</td>\n",
       "      <td>-18.010942</td>\n",
       "      <td>-18.146742</td>\n",
       "      <td>4.725155</td>\n",
       "      <td>-58.193878</td>\n",
       "      <td>39.779388</td>\n",
       "      <td>30.315916</td>\n",
       "      <td>-35.077797</td>\n",
       "      <td>-4.390715</td>\n",
       "      <td>5.400187</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.155847</td>\n",
       "      <td>5.920309</td>\n",
       "      <td>-4.428455</td>\n",
       "      <td>-9.816669</td>\n",
       "      <td>1.046800</td>\n",
       "      <td>-1.762147</td>\n",
       "      <td>0.391905</td>\n",
       "      <td>-4.296741</td>\n",
       "      <td>9.722196</td>\n",
       "      <td>9026.442383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-37.822926</td>\n",
       "      <td>-46.252605</td>\n",
       "      <td>-41.819683</td>\n",
       "      <td>1.036695</td>\n",
       "      <td>-63.024582</td>\n",
       "      <td>3.359123</td>\n",
       "      <td>70.314468</td>\n",
       "      <td>-60.216484</td>\n",
       "      <td>3.915601</td>\n",
       "      <td>9.706659</td>\n",
       "      <td>...</td>\n",
       "      <td>9.645651</td>\n",
       "      <td>-12.054224</td>\n",
       "      <td>2.219497</td>\n",
       "      <td>9.641335</td>\n",
       "      <td>2.633644</td>\n",
       "      <td>-2.663497</td>\n",
       "      <td>-0.842606</td>\n",
       "      <td>9.553460</td>\n",
       "      <td>7.885917</td>\n",
       "      <td>11689.714844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-27.423845</td>\n",
       "      <td>-42.287052</td>\n",
       "      <td>-30.832218</td>\n",
       "      <td>8.538104</td>\n",
       "      <td>2.606682</td>\n",
       "      <td>19.891420</td>\n",
       "      <td>50.305855</td>\n",
       "      <td>-23.230650</td>\n",
       "      <td>0.264111</td>\n",
       "      <td>6.186133</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.108666</td>\n",
       "      <td>-19.899450</td>\n",
       "      <td>-0.874290</td>\n",
       "      <td>15.711926</td>\n",
       "      <td>2.455587</td>\n",
       "      <td>-7.432326</td>\n",
       "      <td>-1.215415</td>\n",
       "      <td>9.201891</td>\n",
       "      <td>15.515057</td>\n",
       "      <td>10209.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-24.053352</td>\n",
       "      <td>-13.136067</td>\n",
       "      <td>5.281594</td>\n",
       "      <td>1.482672</td>\n",
       "      <td>5.103143</td>\n",
       "      <td>14.902792</td>\n",
       "      <td>72.859749</td>\n",
       "      <td>-43.928345</td>\n",
       "      <td>-1.496652</td>\n",
       "      <td>-4.441090</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.682528</td>\n",
       "      <td>8.461659</td>\n",
       "      <td>2.520974</td>\n",
       "      <td>-34.385506</td>\n",
       "      <td>0.504179</td>\n",
       "      <td>5.300302</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>4.309983</td>\n",
       "      <td>10.958882</td>\n",
       "      <td>18138.257812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-20.039921</td>\n",
       "      <td>-12.279404</td>\n",
       "      <td>-6.523460</td>\n",
       "      <td>-1.431632</td>\n",
       "      <td>-75.754738</td>\n",
       "      <td>-17.950811</td>\n",
       "      <td>103.424950</td>\n",
       "      <td>-25.267406</td>\n",
       "      <td>0.932354</td>\n",
       "      <td>12.243988</td>\n",
       "      <td>...</td>\n",
       "      <td>5.001171</td>\n",
       "      <td>10.366087</td>\n",
       "      <td>-5.542226</td>\n",
       "      <td>-6.260996</td>\n",
       "      <td>1.102087</td>\n",
       "      <td>5.685575</td>\n",
       "      <td>1.329227</td>\n",
       "      <td>-3.284338</td>\n",
       "      <td>-3.394474</td>\n",
       "      <td>10898.248047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1         V2         V3        V4         V5         V6  \\\n",
       "0 -32.892689 -18.010942 -18.146742  4.725155 -58.193878  39.779388   \n",
       "1 -37.822926 -46.252605 -41.819683  1.036695 -63.024582   3.359123   \n",
       "2 -27.423845 -42.287052 -30.832218  8.538104   2.606682  19.891420   \n",
       "3 -24.053352 -13.136067   5.281594  1.482672   5.103143  14.902792   \n",
       "4 -20.039921 -12.279404  -6.523460 -1.431632 -75.754738 -17.950811   \n",
       "\n",
       "           V7         V8        V9        V10  ...        V20        V21  \\\n",
       "0   30.315916 -35.077797 -4.390715   5.400187  ... -12.155847   5.920309   \n",
       "1   70.314468 -60.216484  3.915601   9.706659  ...   9.645651 -12.054224   \n",
       "2   50.305855 -23.230650  0.264111   6.186133  ... -10.108666 -19.899450   \n",
       "3   72.859749 -43.928345 -1.496652  -4.441090  ... -30.682528   8.461659   \n",
       "4  103.424950 -25.267406  0.932354  12.243988  ...   5.001171  10.366087   \n",
       "\n",
       "        V22        V23       V24       V25       V26       V27        V28  \\\n",
       "0 -4.428455  -9.816669  1.046800 -1.762147  0.391905 -4.296741   9.722196   \n",
       "1  2.219497   9.641335  2.633644 -2.663497 -0.842606  9.553460   7.885917   \n",
       "2 -0.874290  15.711926  2.455587 -7.432326 -1.215415  9.201891  15.515057   \n",
       "3  2.520974 -34.385506  0.504179  5.300302 -0.001111  4.309983  10.958882   \n",
       "4 -5.542226  -6.260996  1.102087  5.685575  1.329227 -3.284338  -3.394474   \n",
       "\n",
       "         Amount  \n",
       "0   9026.442383  \n",
       "1  11689.714844  \n",
       "2  10209.898438  \n",
       "3  18138.257812  \n",
       "4  10898.248047  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_gan.inference(row = 100  , cuda = cuda, load_path=\"./gan_model.pth\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
