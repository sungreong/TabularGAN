{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('test': conda)"
  },
  "interpreter": {
   "hash": "d3773b654528d79880d3e2b33a682f0f55f4d8370c8ead39985baabf4eb4f1db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Generator\n",
    "* Feature \n",
    "    * Architecture : Any\n",
    "    * 역변환을 통해서 실제값 만들기\n",
    "\n",
    "# Discriminator\n",
    "* Feature \n",
    "    * Architecture : TABNET\n",
    "    * 전처리 되지 않은 데이터를 바로 사용하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm as SN\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"TABNET_simple\")\n",
    "from tab_network import TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  bin_0  bin_1  bin_2 bin_3 bin_4  nom_0      nom_1    nom_2    nom_3  \\\n",
       "0   0      0      0      0     T     Y  Green   Triangle    Snake  Finland   \n",
       "1   1      0      1      0     T     Y  Green  Trapezoid  Hamster   Russia   \n",
       "2   2      0      0      0     F     Y   Blue  Trapezoid     Lion   Russia   \n",
       "3   3      0      1      0     F     Y    Red  Trapezoid    Snake   Canada   \n",
       "4   4      0      0      0     F     N    Red  Trapezoid     Lion   Canada   \n",
       "\n",
       "   ...      nom_9 ord_0        ord_1        ord_2 ord_3 ord_4  ord_5 day  \\\n",
       "0  ...  2f4cb3d51     2  Grandmaster         Cold     h     D     kr   2   \n",
       "1  ...  f83c56c21     1  Grandmaster          Hot     a     A     bF   7   \n",
       "2  ...  ae6800dd0     1       Expert     Lava Hot     h     R     Jc   7   \n",
       "3  ...  8270f0d71     1  Grandmaster  Boiling Hot     i     D     kW   2   \n",
       "4  ...  b164b72a7     1  Grandmaster     Freezing     a     R     qP   7   \n",
       "\n",
       "  month target  \n",
       "0     2      0  \n",
       "1     8      0  \n",
       "2     2      0  \n",
       "3     1      1  \n",
       "4     8      0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>...</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>T</td>\n      <td>Y</td>\n      <td>Green</td>\n      <td>Triangle</td>\n      <td>Snake</td>\n      <td>Finland</td>\n      <td>...</td>\n      <td>2f4cb3d51</td>\n      <td>2</td>\n      <td>Grandmaster</td>\n      <td>Cold</td>\n      <td>h</td>\n      <td>D</td>\n      <td>kr</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>T</td>\n      <td>Y</td>\n      <td>Green</td>\n      <td>Trapezoid</td>\n      <td>Hamster</td>\n      <td>Russia</td>\n      <td>...</td>\n      <td>f83c56c21</td>\n      <td>1</td>\n      <td>Grandmaster</td>\n      <td>Hot</td>\n      <td>a</td>\n      <td>A</td>\n      <td>bF</td>\n      <td>7</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Blue</td>\n      <td>Trapezoid</td>\n      <td>Lion</td>\n      <td>Russia</td>\n      <td>...</td>\n      <td>ae6800dd0</td>\n      <td>1</td>\n      <td>Expert</td>\n      <td>Lava Hot</td>\n      <td>h</td>\n      <td>R</td>\n      <td>Jc</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Red</td>\n      <td>Trapezoid</td>\n      <td>Snake</td>\n      <td>Canada</td>\n      <td>...</td>\n      <td>8270f0d71</td>\n      <td>1</td>\n      <td>Grandmaster</td>\n      <td>Boiling Hot</td>\n      <td>i</td>\n      <td>D</td>\n      <td>kW</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Red</td>\n      <td>Trapezoid</td>\n      <td>Lion</td>\n      <td>Canada</td>\n      <td>...</td>\n      <td>b164b72a7</td>\n      <td>1</td>\n      <td>Grandmaster</td>\n      <td>Freezing</td>\n      <td>a</td>\n      <td>R</td>\n      <td>qP</td>\n      <td>7</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data = pd.read_csv(\"./dataset/only_cat/train.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"target\"],axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.filter(regex=\"bin|month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/envs/test/lib/python3.8/site-packages/pandas/core/frame.py:3191: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "fac_cols = df.columns.tolist()\n",
    "num_cols = []\n",
    "df[fac_cols] = df[fac_cols].astype(\"object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-7-64552185dea0>:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[col] = df[col].fillna(\"VV_likely\")\n<ipython-input-7-64552185dea0>:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[col] = l_enc.fit_transform(df[col].values)\n"
     ]
    }
   ],
   "source": [
    "types = df.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in fac_cols  + num_cols :\n",
    "    if types[col] == 'object' :\n",
    "        l_enc = LabelEncoder()\n",
    "        df[col] = df[col].fillna(\"VV_likely\")\n",
    "        df[col] = l_enc.fit_transform(df[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        df.fillna(df.loc[:, col].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ col for col in df.columns if col in fac_cols + num_cols] \n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(features)\n",
    "output_dim = 1\n",
    "from typing import List, Any, Dict\n",
    "import torch\n",
    "n_d: int = 5\n",
    "n_a: int = 5\n",
    "n_steps: int = 5\n",
    "gamma: float = 1.3\n",
    "cat_idxs: List[int] =[]\n",
    "cat_dims: List[int] =[]\n",
    "cat_emb_dim: int = 3\n",
    "n_independent: int = 2\n",
    "n_shared: int = 3\n",
    "epsilon: float = 1e-15\n",
    "momentum: float = 0.02\n",
    "lambda_sparse: float = 1e-3\n",
    "seed: int = 2345\n",
    "clip_value: int = 1\n",
    "verbose: int = 1\n",
    "optimizer_fn: Any = torch.optim.Adam\n",
    "optimizer_params: Dict = dict(lr=2e-2)\n",
    "scheduler_fn: Any = None\n",
    "scheduler_params: Dict = {\"step_size\":50,\"gamma\":0.9}\n",
    "mask_type: str = \"entmax\" # \"sparsemax\"\n",
    "input_dim: int = input_dim\n",
    "output_dim: int = output_dim\n",
    "device_name: str = \"auto\"\n",
    "virtual_batch_size=128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   bin_0  bin_1  bin_2  bin_3  bin_4  month\n",
       "0      0      0      0      1      1      1\n",
       "1      0      1      0      1      1      7\n",
       "2      0      0      0      0      1      1\n",
       "3      0      1      0      0      1      0\n",
       "4      0      0      0      0      0      7"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import category_encoders as ce \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/envs/test/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        bin_0  bin_1  bin_2  bin_3  bin_4  month\n",
       "0           0      0      0      1      1      1\n",
       "1           0      1      0      1      1      7\n",
       "2           0      0      0      0      1      1\n",
       "3           0      1      0      0      1      0\n",
       "4           0      0      0      0      0      7\n",
       "...       ...    ...    ...    ...    ...    ...\n",
       "299995      0      0      0      1      0      7\n",
       "299996      0      0      0      0      1      1\n",
       "299997      0      0      0      0      1      8\n",
       "299998      0      1      0      0      1      7\n",
       "299999      0      0      0      0      1      2\n",
       "\n",
       "[300000 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>299995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>299996</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>299997</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>299998</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>299999</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "encoder = ce.OneHotEncoder(cols=fac_cols, use_cat_names= True ,handle_unknown=\"ignore\",)\n",
    "encoder.fit(df)\n",
    "df_trans = encoder.transform(df)\n",
    "encoder.inverse_transform(df_trans)"
   ]
  },
  {
   "source": [
    "df_trans\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import umap"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-270fe150f469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2345\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/test/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m             self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0m\u001b[1;32m   2554\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# JH why raw data?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/envs/test/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \u001b[0mreplaced\u001b[0m \u001b[0mby\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m         \"\"\"\n\u001b[0;32m-> 2580\u001b[0;31m         return simplicial_set_embedding(\n\u001b[0m\u001b[1;32m   2581\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/test/lib/python3.8/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meuclidean_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m         embedding = optimize_layout_euclidean(\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/test/lib/python3.8/site-packages/umap/layouts.py\u001b[0m in \u001b[0;36moptimize_layout_euclidean\u001b[0;34m(head_embedding, tail_embedding, head, tail, n_epochs, n_vertices, epochs_per_sample, a, b, rng_state, gamma, initial_alpha, negative_sample_rate, parallel, verbose, densmap, densmap_kwds)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mdens_re_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         optimize_fn(\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0mhead_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mtail_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reducer = umap.UMAP(random_state=2345)\n",
    "reducer.fit(df_trans.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(df_trans.values)\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "map_info = deepcopy(encoder.category_mapping)\n",
    "[_dict_.update({\"len\" : len(_dict_[\"mapping\"])-1}) for _dict_ in map_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import gumbel_softmax as g_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit2onehot(logit, map_info) :\n",
    "    cur_position = 0\n",
    "    result = []\n",
    "    for col_info in map_info :\n",
    "        cur_position += col_info[\"len\"]\n",
    "        tt = logit[:, (cur_position-col_info[\"len\"]):cur_position]\n",
    "        onehot_ = g_softmax(logits= tt , hard=True,tau=1)\n",
    "        result.append(onehot_)\n",
    "    return torch.cat(result,axis=1)\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "def logit2dist(logit,map_info) :\n",
    "    cur_position = 0\n",
    "    result = []\n",
    "    for col_info in map_info :\n",
    "        cur_position += col_info[\"len\"]\n",
    "        tt = logit[:, (cur_position-col_info[\"len\"]):cur_position]\n",
    "        dist = Categorical(logits = tt)\n",
    "        result.append(dist)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['bin_0_0.0',\n",
       " 'bin_0_1.0',\n",
       " 'bin_1_0.0',\n",
       " 'bin_1_1.0',\n",
       " 'bin_2_0.0',\n",
       " 'bin_2_1.0',\n",
       " 'bin_3_1.0',\n",
       " 'bin_3_0.0',\n",
       " 'bin_4_1.0',\n",
       " 'bin_4_0.0',\n",
       " 'month_1.0',\n",
       " 'month_7.0',\n",
       " 'month_0.0',\n",
       " 'month_3.0',\n",
       " 'month_9.0',\n",
       " 'month_2.0',\n",
       " 'month_6.0',\n",
       " 'month_8.0',\n",
       " 'month_11.0',\n",
       " 'month_10.0',\n",
       " 'month_4.0',\n",
       " 'month_5.0']"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "total_column = df_trans.columns.tolist()\n",
    "total_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass, field\n",
    "# @dataclass\n",
    "# class Book :\n",
    "#     n_d: int\n",
    "#     n_a: int = 8\n",
    "#     n_steps: int = 3\n",
    "\n",
    "#     def __post_init__(self,data) :\n",
    "#         self.full = self.n_d + self.n_a\n",
    "# Book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataModule(pl.LightningDataModule) :\n",
    "    def __init__(self, data , batch_size:int = 32 , num_workers:int=3) :\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.dims = self.data.shape[1]\n",
    "    \n",
    "    def prepare_data(self,) :\n",
    "        pass\n",
    "\n",
    "    def setup(self,stage=None) :\n",
    "        if stage == \"fit\" or state is None :\n",
    "            train_length = int(len(self.data)*0.8)\n",
    "            lengths = [train_length, int(len(self.data)-train_length)]\n",
    "            self.train, self.val = random_split(self.data, lengths)\n",
    "        \n",
    "        if stage == \"test\" or stage is None :\n",
    "            self.test = self.data   \n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train , batch_size= self.batch_size , num_workers=self.num_workers )\n",
    "\n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.val , batch_size= self.batch_size , num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test , batch_size= self.batch_size , num_workers=self.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, out_shape, map_info , encoder, total_column):\n",
    "        super().__init__()\n",
    "        self.out_shape = out_shape\n",
    "        self.map_info = map_info\n",
    "        self.encoder = encoder\n",
    "        self.total_column = total_column\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=True),\n",
    "            *block(128, 128, normalize=True),\n",
    "            *block(128, 64, normalize=True),\n",
    "            *block(64, 32, normalize=True),\n",
    "            nn.Linear(32, self.out_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.model(z)\n",
    "        # img = img.view(img.size(0), *self.out_shape)\n",
    "        return x\n",
    "\n",
    "    def inference(self,z) :\n",
    "        x = self.model(z)\n",
    "        x = self.logit2onehot(x)\n",
    "        return self.onehot2rev(x)\n",
    "\n",
    "    def inference_tabnet_training(self,z):\n",
    "        return torch.FloatTensor(self.inference(z).values)\n",
    "\n",
    "    def splitbycolumn(self,logit) :\n",
    "        cur_position = 0\n",
    "        result = []\n",
    "        for col_info in self.map_info :\n",
    "            cur_position += col_info[\"len\"]\n",
    "            tt = logit[:, (cur_position-col_info[\"len\"]):cur_position]\n",
    "            result.append(tt)\n",
    "        return result\n",
    "\n",
    "    def logit2index(self,logit) :\n",
    "        cur_position = 0\n",
    "        result = []\n",
    "        for col_info in self.map_info :\n",
    "            cur_position += col_info[\"len\"]\n",
    "            tt = logit[:, (cur_position-col_info[\"len\"]):cur_position]\n",
    "            onehot_ = g_softmax(logits= tt , hard=True,tau=1)\n",
    "            result.append(torch.argmax(tt,axis=1))\n",
    "        return result\n",
    "\n",
    "    def logit2onehot(self,logit) :\n",
    "        cur_position = 0\n",
    "        result = []\n",
    "        for col_info in self.map_info :\n",
    "            cur_position += col_info[\"len\"]\n",
    "            tt = logit[:, (cur_position-col_info[\"len\"]):cur_position]\n",
    "            onehot_ = g_softmax(logits= tt , hard=True,tau=1)\n",
    "            result.append(onehot_)\n",
    "        return torch.cat(result,axis=1)\n",
    "    \n",
    "    def logit2dist(self,logit) :\n",
    "        cur_position = 0\n",
    "        result = []\n",
    "        for col_info in self.map_info :\n",
    "            cur_position += col_info[\"len\"]\n",
    "            tt = logit[:, (cur_position-col_info[\"len\"]):cur_position]\n",
    "            dist = Categorical(logits = tt)\n",
    "            result.append(dist)\n",
    "        return result\n",
    "\n",
    "    def onehot2rev(self,tensor) :\n",
    "        gene_onehot_pd = pd.DataFrame(tensor.detach().numpy(),\n",
    "        columns=self.total_column)\n",
    "        gene_data = self.encoder.inverse_transform(gene_onehot_pd)\n",
    "        return gene_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=30, out_shape=len(total_column),\n",
    "map_info=map_info,encoder=encoder , total_column=total_column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    bin_0  bin_1  bin_2  bin_3  bin_4  month\n",
       "0       1      0      1      1      1      9\n",
       "1       0      1      1      1      1      6\n",
       "2       1      0      1      0      1     10\n",
       "3       0      1      1      0      1     10\n",
       "4       0      0      0      0      1      0\n",
       "..    ...    ...    ...    ...    ...    ...\n",
       "95      0      1      1      1      0      0\n",
       "96      1      1      0      0      1      8\n",
       "97      1      0      1      0      0      2\n",
       "98      0      1      1      0      1      1\n",
       "99      0      1      1      0      0      6\n",
       "\n",
       "[100 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "z = torch.randn(100, 30)\n",
    "gene_logit = generator(z)\n",
    "gene_onehot_tensor = logit2onehot(gene_logit, map_info)\n",
    "gene_onehot_pd = pd.DataFrame(gene_onehot_tensor.detach().numpy(),columns=total_column)\n",
    "gene_data = encoder.inverse_transform(gene_onehot_pd)\n",
    "gene_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(100, 30)\n",
    "# generator.inference(z)\n",
    "# generator.splitbycolumn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor(0.6880, grad_fn=<MeanBackward0>),\n",
       " tensor(0.6881, grad_fn=<MeanBackward0>),\n",
       " tensor(0.6929, grad_fn=<MeanBackward0>),\n",
       " tensor(0.6881, grad_fn=<MeanBackward0>),\n",
       " tensor(0.6927, grad_fn=<MeanBackward0>),\n",
       " tensor(2.4807, grad_fn=<MeanBackward0>)]"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "[dist.entropy().mean() for dist in logit2dist(gene_logit , map_info)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "Discriminator = partial(TabNet, \n",
    "            output_dim = 1,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            mask_type=mask_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc = Discriminator(dm.size())\n",
    "# disc(torch.FloatTensor(generator.inference(z).values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule): \n",
    "    def __init__(self,\n",
    "                gene_output_dim = None,\n",
    "                disc_input_dim = None,\n",
    "                latent_dim =100 , \n",
    "                lr: float = 0.0002,\n",
    "                b1: float = 0.5,\n",
    "                b2: float = 0.999,\n",
    "                generator_parm:dict={},\n",
    "                discriminator_parm:dict={},\n",
    "                # batch_size: int = 64,\n",
    "                **kwargs) :\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.generator = Generator(latent_dim=self.hparams.latent_dim,\n",
    "        out_shape=self.hparams.gene_output_dim,**generator_parm)\n",
    "\n",
    "        self.discriminator = Discriminator(input_dim=self.hparams.disc_input_dim,**discriminator_parm)\n",
    "\n",
    "        self.validation_z = torch.randn(8, self.hparams.latent_dim)\n",
    "\n",
    "        self.example_input_array = torch.zeros(2, self.hparams.latent_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def inference(self,z) :\n",
    "        return self.generator.inference(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        z = torch.randn(x.shape[0], self.hparams.latent_dim)\n",
    "        z = z.type_as(x)\n",
    "        generated_x = self.generator.inference_tabnet_training(z)\n",
    "        valid = torch.ones(x.size(0), 1)\n",
    "        valid = valid.type_as(x)\n",
    "        logit,M_loss = self.discriminator(generated_x)\n",
    "        reward = F.binary_cross_entropy(nn.Sigmoid()(logit), valid,reduction ='mean').detach()\n",
    "        self.log('val_reward', reward)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        x = batch\n",
    "        # sample noise\n",
    "        z = torch.randn(x.shape[0], self.hparams.latent_dim)\n",
    "        z = z.type_as(x)\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            ## TODO: how to training generator? \n",
    "            ## TODO: 현재 강화학습 방식을 도입하여 유사한 로스를 설계해서 진행중...(어디에도 없는 방식이라 확신 없음)\n",
    "            \n",
    "            # generate images\n",
    "            self.generated_imgs = self.generator.inference_tabnet_training(z)\n",
    "\n",
    "            # ground truth result (ie: all fake)\n",
    "            # put on GPU because we created this tensor inside training_loop\n",
    "            valid = torch.ones(x.size(0), 1)\n",
    "            valid = valid.type_as(x)\n",
    "\n",
    "            # adversarial loss is binary cross-entropy\n",
    "            logit,M_loss = self.discriminator(self.generated_imgs)\n",
    "            # g_loss = self.adversarial_loss(nn.Sigmoid()(logit), valid).detach()\n",
    "            reward = 1/F.binary_cross_entropy(nn.Sigmoid()(logit), valid,reduction ='none').detach()\n",
    "            logit = self.generator(z)\n",
    "            list_of_dist = self.generator.logit2dist(logit)\n",
    "            list_of_action = self.generator.logit2index(logit)\n",
    "            g_loss_list = [(dist.log_prob(action) * reward.squeeze()).mean() for dist , action in zip(list_of_dist,list_of_action)]\n",
    "            # print(g_loss)\n",
    "            g_loss = torch.stack(g_loss_list).mean()\n",
    "            tqdm_dict = {'g_loss': g_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': g_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            # how well can it label as real?\n",
    "            valid = torch.ones(x.size(0), 1)\n",
    "            valid = valid.type_as(x)\n",
    "            logit,M_loss = self.discriminator(x)\n",
    "            real_loss = self.adversarial_loss(nn.Sigmoid()(logit), valid)\n",
    "\n",
    "            # how well can it label as fake?\n",
    "            fake = torch.zeros(x.size(0), 1)\n",
    "            fake = fake.type_as(x)\n",
    "            logit,M_loss =self.discriminator(self.generator.inference_tabnet_training(z).detach())\n",
    "            fake_loss = self.adversarial_loss(nn.Sigmoid()(logit) , fake)\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            tqdm_dict = {'d_loss': d_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': d_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generator.model[0].weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_parm = dict(map_info=map_info,encoder=encoder,total_column = total_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TabularDataModule(df.values.astype(np.float32),batch_size=1024)\n",
    "model = GAN(gene_output_dim = len(total_column), disc_input_dim = dm.size(), generator_parm = generator_parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/opt/conda/envs/test/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name          | Type      | Params | In sizes | Out sizes\n",
      "-------------------------------------------------------------------\n",
      "0 | generator     | Generator | 41.2 K | [2, 100] | [2, 22]  \n",
      "1 | discriminator | TabNet    | 4.4 K  | ?        | ?        \n",
      "-------------------------------------------------------------------\n",
      "45.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "45.6 K    Total params\n",
      "0.182     Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38e84e026a9e49b1a05d576fc69f373c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b06f3b6306ea4107b0795570c226cf53"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=10, progress_bar_refresh_rate=20)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"./cat_gan_model.ckpt\")\n",
    "new_model = model.load_from_checkpoint(checkpoint_path=\"./cat_gan_model.ckpt\")\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=100, progress_bar_refresh_rate=20)\n",
    "trainer.fit(new_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/envs/test/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/opt/conda/envs/test/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name          | Type      | Params | In sizes | Out sizes\n",
      "-------------------------------------------------------------------\n",
      "0 | generator     | Generator | 41.2 K | [2, 100] | [2, 22]  \n",
      "1 | discriminator | TabNet    | 4.4 K  | ?        | ?        \n",
      "-------------------------------------------------------------------\n",
      "45.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "45.6 K    Total params\n",
      "0.182     Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4723019b29104847aef97ceb654665cd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d19dd61d03af4248b8a088b08df75a20"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 0, global step 234: val_reward was not in top 1\n",
      "Epoch 1, global step 469: val_reward was not in top 1\n",
      "/opt/conda/envs/test/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "new_model = model.load_from_checkpoint(checkpoint_path=\"./cat_gan_model.ckpt\")\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint , EarlyStopping\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_reward',\n",
    "    dirpath='./model',\n",
    "    verbose=True,\n",
    "    filename='gan-{epoch:02d}-{val_reward:.2f}',\n",
    "    save_top_k=1, # 3,\n",
    "    # save_last=True,\n",
    "    mode='min',\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_reward',\n",
    "    patience=100,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=100, progress_bar_refresh_rate=20,callbacks=[checkpoint_callback,early_stopping])\n",
    "trainer.fit(new_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   bin_0  bin_1  bin_2 bin_3 bin_4  month\n",
       "0      0      0      1     F     Y      9\n",
       "1      0      0      1     F     N      7\n",
       "2      1      0      0     F     Y      8\n",
       "3      0      0      1     F     Y      9\n",
       "4      0      0      1     T     N      9\n",
       "5      1      0      0     F     Y      2\n",
       "6      0      0      0     T     Y      1\n",
       "7      0      0      1     F     Y      8\n",
       "8      1      0      1     T     Y      8\n",
       "9      0      0      1     T     N      5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>F</td>\n      <td>N</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>T</td>\n      <td>N</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>T</td>\n      <td>Y</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>T</td>\n      <td>Y</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>T</td>\n      <td>N</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "z = torch.randn(10, model.hparams.latent_dim)\n",
    "model.inference(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}